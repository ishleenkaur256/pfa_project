{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1a6YRlmBYIvdlIoA8qygE37EGR6od-wGW",
      "authorship_tag": "ABX9TyOHPRVmINBa7D3g9SpEB7y3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishleenkaur256/pfa_project/blob/main/Copy_of_PFA_Final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import bs4\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib import ticker\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "import operator\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import Image\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy\n",
        "import random\n",
        "seed = 42\n",
        "numpy.random.seed(42)\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from sklearn import  svm"
      ],
      "metadata": {
        "id": "cp5RvQB-Ee1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqmHAX5fEVM3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "import random\n",
        "\n",
        "\n",
        "CATEGORIES = {\n",
        "    \"NATIONAL\": \"https://www.thehindu.com/news/national/\",\n",
        "    \"BUSINESS\": \"https://www.thehindu.com/business/\",\n",
        "    \"LIFE-STYLE\": \"https://www.thehindu.com/life-and-style/\",\n",
        "    \"TECHNOLOGY\": \"https://www.thehindu.com/sci-tech/technology/\",\n",
        "    \"EDUCATION\": \"https://www.thehindu.com/education/\",\n",
        "}\n",
        "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "OUTPUT_FILENAME = '/content/drive/MyDrive/PFA_Final_Project/thehindu_dataset.csv'\n",
        "scraped_articles = []\n",
        "\n",
        "BASE_DELAY = 1.0\n",
        "MAX_CONTAINERS_PER_PAGE = 50\n",
        "PAGES_TO_SCRAPE = 10\n",
        "\n",
        "\n",
        "def get_summary_from_article_page(article_link, headers):\n",
        "    \"\"\"Fetches the full article page and returns the first paragraph (the summary).\"\"\"\n",
        "    summary = \"Summary extraction failed\"\n",
        "    try:\n",
        "        deep_response = requests.get(article_link, headers=headers, timeout=10)\n",
        "        deep_response.raise_for_status()\n",
        "        deep_soup = BeautifulSoup(deep_response.text, 'html.parser')\n",
        "\n",
        "        # Target the main content wrapper\n",
        "        article_body_wrapper = deep_soup.find('div', class_=re.compile(r'article-body|content-body|body-text|main-article-content|story-text'))\n",
        "\n",
        "        if article_body_wrapper:\n",
        "            first_paragraph = article_body_wrapper.find('p')\n",
        "            if first_paragraph:\n",
        "                summary = first_paragraph.get_text(strip=True)[:200] + \"...\"\n",
        "                return summary\n",
        "\n",
        "        fallback_paragraph = deep_soup.find('div', id=re.compile(r'content')).find('p')\n",
        "        if fallback_paragraph:\n",
        "             summary = fallback_paragraph.get_text(strip=True)[:200] + \"...\"\n",
        "             return summary\n",
        "\n",
        "    except requests.exceptions.RequestException:\n",
        "        pass\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "print(f\"--- Starting Scraping: {PAGES_TO_SCRAPE} Pages x {len(CATEGORIES)} Categories ---\")\n",
        "total_new_articles = 0\n",
        "\n",
        "for category_name, base_url in CATEGORIES.items():\n",
        "\n",
        "    # Print only once per category loop\n",
        "    print(f\"\\n--- Processing Category: {category_name} (Pages 1 to {PAGES_TO_SCRAPE}) ---\")\n",
        "\n",
        "    for page_num in range(1, PAGES_TO_SCRAPE + 1):\n",
        "\n",
        "        # 1. Construct the URL\n",
        "        if page_num == 1:\n",
        "            category_url = base_url\n",
        "        else:\n",
        "            category_url = f\"{base_url}?page={page_num}\"\n",
        "\n",
        "        try:\n",
        "            # 2. Fetch the page\n",
        "            response = requests.get(category_url, headers=HEADERS, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # 3. Extract article containers\n",
        "            article_containers = soup.find_all('div', class_=re.compile(r'article-card|story-card|list-item'))\n",
        "            if not article_containers:\n",
        "                article_containers = soup.find_all(['h3', 'h2'], class_=re.compile(r''))\n",
        "                article_containers = [tag for tag in article_containers if tag.find('a')]\n",
        "\n",
        "            articles_processed_in_page = 0\n",
        "\n",
        "            # 4. Deep Dive Loop\n",
        "            for container in article_containers:\n",
        "                if articles_processed_in_page >= MAX_CONTAINERS_PER_PAGE:\n",
        "                    break\n",
        "\n",
        "                link_tag = container.find('a', href=True)\n",
        "                if not link_tag: continue\n",
        "\n",
        "                title = link_tag.get_text(strip=True)\n",
        "                link = urljoin(category_url, link_tag['href'])\n",
        "\n",
        "                # Filtering and Duplicate Check\n",
        "                if not title or len(title) < 10 or 'subscription' in link.lower() or 'opinion' in link.lower(): continue\n",
        "                if any(article['Link'] == link for article in scraped_articles): continue # Skip duplicates\n",
        "\n",
        "                # Execute Deep Dive\n",
        "                delay_time = BASE_DELAY + random.uniform(0.1, 0.5)\n",
        "                time.sleep(delay_time)\n",
        "\n",
        "                summary = get_summary_from_article_page(link, HEADERS)\n",
        "\n",
        "                scraped_articles.append({\n",
        "                    'Title': title,\n",
        "                    'Summary': summary,\n",
        "                    'Category': category_name,\n",
        "                    'Link': link\n",
        "                })\n",
        "\n",
        "                articles_processed_in_page += 1\n",
        "                total_new_articles += 1\n",
        "\n",
        "            # Print minimal status update only if articles were added\n",
        "            if articles_processed_in_page > 0:\n",
        "                print(f\"  > Successfully added {articles_processed_in_page} articles from Page {page_num}.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"FAILED to fetch {category_name} Page {page_num}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred in {category_name} Page {page_num}: {e}\")\n",
        "\n",
        "# --- Final Output and CSV Saving ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"Scraping Complete! Total unique articles collected: {len(scraped_articles)}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df = pd.DataFrame(scraped_articles)\n",
        "df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Data saved to **{OUTPUT_FILENAME}**\")\n",
        "\n",
        "if not df.empty:\n",
        "    print(f\"Sample of final data (first 5 rows):\\n{df.head().to_markdown(index=False)}\")\n",
        "else:\n",
        "    print(\"No articles were successfully extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/drive/MyDrive/PFA_Final_Project/thehindu_dataset.csv\")"
      ],
      "metadata": {
        "id": "qr60u664Em5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "S5P-C1VgIKvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text\"] = df[\"Title\"].astype(str) + \" \" + df[\"Summary\"].astype(str)\n"
      ],
      "metadata": {
        "id": "fdlHx_q3IKsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "22pJsKZhIKmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"Category\"])\n"
      ],
      "metadata": {
        "id": "q47oeQCRIKix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "YkACJATHIKgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n"
      ],
      "metadata": {
        "id": "kyMMptbaIKdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "14-p6D5TIKa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_remove = [\"Title\", \"Summary\", \"Category\", \"Link\"]\n",
        "train_dataset = train_dataset.remove_columns(cols_to_remove)\n",
        "test_dataset   = test_dataset.remove_columns(cols_to_remove)\n"
      ],
      "metadata": {
        "id": "Ly5nStA4IKVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.column_names\n"
      ],
      "metadata": {
        "id": "ZuJxTI2wJE5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.column_names"
      ],
      "metadata": {
        "id": "rozulcIjkvLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"albert-base-v2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "num_labels = df[\"label\"].nunique()\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "psa9wf7BT3cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "HnhOrwi7T5S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
        ")\n",
        "\n",
        "test_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "w2tJ3vocT7zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/PFA_Final_Project/albert-news-model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=20,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "4ndKtm0kT-UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "zRBJ0GrEUA2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "5C3f7qCuUDl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "preds = predictions.predictions.argmax(axis=-1)\n",
        "true = predictions.label_ids\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(true, preds))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(true, preds, target_names=le.classes_))\n"
      ],
      "metadata": {
        "id": "SmnVvP2EUFj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(true, preds)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=le.classes_,\n",
        "            yticklabels=le.classes_)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cKFePaNgeZCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def predict_news_category(title, summary):\n",
        "    # Combine training dataset\n",
        "    text = title + \" \" + summary\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get predicted class ID\n",
        "    predicted_label_id = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    # Convert number back to label name\n",
        "    predicted_label = le.inverse_transform([predicted_label_id])[0]\n",
        "\n",
        "    return predicted_label\n"
      ],
      "metadata": {
        "id": "DD5ni2-Me5MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_news_category(\n",
        "    \"Sensex rises 350 points amid global market optimism\",\n",
        "    \"Investors reacted positively to corporate earnings and global cues.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "cNAUuaFGfzP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_news_category(\n",
        "    \"ISRO successfully tests new space propulsion system\",\n",
        "    \"The breakthrough could enhance satellite maneuvering and deep-space missions.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "oJCtE0Hxf-sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/PFA_Final_Project/news_classifier_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/PFA_Final_Project/news_classifier_model\")\n"
      ],
      "metadata": {
        "id": "kq1Tm33qgCtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/PFA_Final_Project/label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(le, f)\n"
      ],
      "metadata": {
        "id": "pmHhW1HfgK2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio"
      ],
      "metadata": {
        "id": "-8P1Fw7s_mfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "id": "nw7FEMj-gMz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "RlvAdOHHW8Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Load saved model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/PFA_Final_Project/news_classifier_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/PFA_Final_Project/news_classifier_model\")\n",
        "\n",
        "# Load label encoder\n",
        "with open(\"/content/drive/MyDrive/PFA_Final_Project/label_encoder.pkl\", \"rb\") as f:\n",
        "    le = pickle.load(f)\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "9elMRyKigpkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_news(title, summary):\n",
        "    text = title + \" \" + summary\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    pred_id = torch.argmax(outputs.logits, dim=1).item()\n",
        "    label = le.inverse_transform([pred_id])[0]\n",
        "\n",
        "    return label\n"
      ],
      "metadata": {
        "id": "0kYeYq6igsa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def classify_csv(file):\n",
        "    df = pd.read_csv(file.name)\n",
        "\n",
        "    # Check required columns\n",
        "    if not {\"Title\", \"Summary\"} <= set(df.columns):\n",
        "        return \"CSV must contain 'Title' and 'Summary' columns.\", None\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        title = str(df.iloc[i][\"Title\"])\n",
        "        summary = str(df.iloc[i][\"Summary\"])\n",
        "\n",
        "        pred = classify_news(title, summary)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    df[\"Predicted_Category\"] = predictions\n",
        "\n",
        "    # Save output CSV\n",
        "    output_path = \"/content/drive/MyDrive/PFA_Final_Project/predicted_results.csv\"\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    return df, output_path\n"
      ],
      "metadata": {
        "id": "7ZChZiRxjTPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_css = custom_css = \"\"\"\n",
        "/* Background Image */\n",
        ".gradio-container {\n",
        "    background: url('https://images.unsplash.com/photo-1507842217343-583bb7270b66?auto=format&fit=crop&w=1350&q=80') no-repeat center center fixed ;\n",
        "    background-size: cover !important;\n",
        "    font-family: 'Poppins', sans-serif;\n",
        "    position: relative;\n",
        "    z-index: 1;\n",
        "}\n",
        "\n",
        ".gradio-container::before {\n",
        "    content: \"\";\n",
        "    position: absolute;\n",
        "    top: 0;\n",
        "    left: 0;\n",
        "    height: 100%;\n",
        "    width: 100%;\n",
        "    background: rgba(0,0,0,0.55);\n",
        "    z-index: 1;\n",
        "    pointer-events: none;\n",
        "}\n",
        "\n",
        "\n",
        ".gradio-container * {\n",
        "    position: relative;\n",
        "    z-index: 3;\n",
        "    color: #4e2a0a !important;\n",
        "}\n",
        "\n",
        "\n",
        ".prose.svelte-ydeks8 h1,\n",
        "h1.svelte-ydeks8,\n",
        ".prose h1 {\n",
        "    color: #ffffff ;\n",
        "    font-size: 48px ;\n",
        "    font-weight: 800 ;\n",
        "    text-align: center ;\n",
        "\n",
        "    text-shadow:\n",
        "        0 0 10px rgba(255, 220, 160, 0.9),\n",
        "        0 0 20px rgba(255, 180, 90, 0.8),\n",
        "        0 0 40px rgba(255, 150, 50, 0.7),\n",
        "        0 0 60px rgba(255, 120, 30, 0.6);\n",
        "\n",
        "    animation: headingGlow 2.5s infinite alternate ease-in-out;\n",
        "}\n",
        "\n",
        "@keyframes headingGlow {\n",
        "    from {\n",
        "        text-shadow:\n",
        "            0 0 6px rgba(255, 220, 160, 0.7),\n",
        "            0 0 14px rgba(255, 180, 90, 0.6),\n",
        "            0 0 25px rgba(255, 150, 50, 0.4);\n",
        "    }\n",
        "    to {\n",
        "        text-shadow:\n",
        "            0 0 12px rgba(255, 240, 200, 1),\n",
        "            0 0 28px rgba(255, 200, 120, 0.9),\n",
        "            0 0 45px rgba(255, 160, 80, 0.7),\n",
        "            0 0 70px rgba(255, 140, 60, 0.6);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        ".typing {\n",
        "    width: 100%;\n",
        "    border-right: 3px solid white;\n",
        "    white-space: nowrap;\n",
        "    overflow: hidden;\n",
        "    animation: typing 4s steps(30), blink 0.75s step-end infinite;\n",
        "}\n",
        "@keyframes typing {\n",
        "    from { width: 0 }\n",
        "    to   { width: 100% }\n",
        "}\n",
        "@keyframes blink {\n",
        "    from, to { border-color: transparent }\n",
        "    50% { border-color: white }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "button {\n",
        "    background: linear-gradient(90deg, #b38752, #84592c) ;\n",
        "    color: #fff7e6 ;\n",
        "    font-weight: bold ;\n",
        "    border-radius: 12px ;\n",
        "    padding: 10px 20px ;\n",
        "    font-size: 17px ;\n",
        "}\n",
        "button:hover {\n",
        "    transform: scale(1.05);\n",
        "    background: linear-gradient(90deg, #c99a60, #966736) ;\n",
        "}\n",
        "\n",
        "label, .gr-label, .form label {\n",
        "    color: #e6cba8 ;\n",
        "    font-weight: 700 ;\n",
        "    font-size: 18px ;\n",
        "}\n",
        "\n",
        "\n",
        ".footer {\n",
        "    margin-top: 20px;\n",
        "    text-align: center;\n",
        "    color: #f2f2f2;\n",
        "    font-size: 16px;\n",
        "    font-weight: 600;\n",
        "    text-shadow: 1px 1px 2px black;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        ".gradio-tabs {\n",
        "    background: transparent ;\n",
        "}\n",
        "\n",
        "\n",
        ".gradio-tabs .tabitem span {\n",
        "    color: white ;\n",
        "}\n",
        "/* Gradient brown input boxes */\n",
        "input[type='text'], textarea {\n",
        "    background: linear-gradient(180deg, rgba(214,180,138,0.25), rgba(180,140,95,0.30)) !important;\n",
        "    border: 2px solid #b38752 !important;\n",
        "    border-radius: 12px !important;\n",
        "    padding: 12px !important;\n",
        "    font-size: 16px !important;\n",
        "    color: white !important;\n",
        "    backdrop-filter: blur(4px);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        ".gradio-tabs .tabitem.selected {\n",
        "    background: linear-gradient(90deg, #b38752, #84592c) ;\n",
        "    border-radius: 10px ;\n",
        "    border-bottom: none ;\n",
        "}\n",
        "\n",
        "\n",
        ".gradio-tabs .tabitem:hover {\n",
        "    background: rgba(255, 255, 255, 0.12) ;\n",
        "    border-radius: 10px ;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        ".block-label:hover {\n",
        "    box-shadow: 0 0 10px rgba(214,180,138,0.6);\n",
        "}\n",
        "\n",
        "span[class^=\"svelte-\"][data-testid=\"block-info\"] {\n",
        "    background-color: rgba(214,180,138,0.40) !important;\n",
        "    color: #4e2a0a !important;\n",
        "    padding: 6px 12px !important;\n",
        "    border-radius: 8px !important;\n",
        "    border: 1px solid #b38752 !important;\n",
        "    font-weight: 700 !important;\n",
        "    font-size: 16px !important;\n",
        "    box-shadow: 0 0 6px rgba(0,0,0,0.25);\n",
        "}\n",
        "\n",
        "label.svelte-j0zqjt.float,\n",
        "label.svelte-j0zqjt,\n",
        "label[class*=\"svelte-\"].float {\n",
        "    background-color: rgba(214,180,138,0.40) ;\n",
        "    color: #4e2a0a !important;\n",
        "    padding: 6px 12px !important;\n",
        "    border-radius: 8px !important;\n",
        "    border: 1px solid #b38752 !important;\n",
        "    font-weight: 700 !important;\n",
        "    font-size: 18px ;\n",
        "    box-shadow: 0 0 6px rgba(0,0,0,0.25);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "button.svelte-edrmkl.center.boundedheight.flex {\n",
        "    background: rgba(240, 220, 190, 0.35) ;\n",
        "    border: 2px solid #b38752 ;\n",
        "    border-radius: 15px ;\n",
        "    backdrop-filter: blur(4px);\n",
        "    padding: 20px ;\n",
        "}\n",
        "\n",
        "/* Text inside the upload box */\n",
        "button.svelte-edrmkl.center.boundedheight.flex * {\n",
        "    color: #4e2a0a !important;\n",
        "    font-weight: 600 !important;\n",
        "}\n",
        "\n",
        "/* Upload SVG icon (arrow) */\n",
        "button.svelte-edrmkl.center.boundedheight.flex svg {\n",
        "    stroke: #4e2a0a !important;\n",
        "}\n",
        "\n",
        "\n",
        "button.svelte-i00v67.selected::after {\n",
        "    background: linear-gradient(90deg, #b38752, #84592c) !important;\n",
        "    height: 4px !important;  /* adjust thickness */\n",
        "    bottom: -2px !important;\n",
        "    border-radius: 2px;\n",
        "}\n",
        "\n",
        "label[for*=\"file-download\"] + div {\n",
        "    background: linear-gradient(180deg, #ecd7bd, #d6b48a) !important;\n",
        "    border: 2px solid #b38752 !important;\n",
        "    border-radius: 14px !important;\n",
        "    padding: 20px !important;\n",
        "    backdrop-filter: blur(4px);\n",
        "}\n",
        "\n",
        "\n",
        ".input-container div,\n",
        ".input-container .svelte-1ae7ssi,\n",
        ".input-container *:not(input):not(textarea) {\n",
        "    color: #5a3b15 !important;\n",
        "}\n",
        "\n",
        ".input-container input {\n",
        "    caret-color: #5a3b15 !important;\n",
        "}\n",
        "\n",
        "\n",
        ".typing {\n",
        "    color: #ffffff !important;\n",
        "    text-shadow: 0 0 4px rgba(0,0,0,0.6) !important;\n",
        "}\n",
        "\n",
        "\n",
        "div.prose.svelte-ydeks8,\n",
        "div.prose.svelte-ydeks8 * {\n",
        "    color: #ffffff !important;\n",
        "    text-shadow: 0 0 4px rgba(0,0,0,0.6) !important;\n",
        "}\n",
        "\n",
        "\n",
        ".prose.svelte-ydeks8 h1,\n",
        "div.prose.svelte-ydeks8 h1,\n",
        ".prose h1 {\n",
        "    color: #ffffff !important;\n",
        "    text-align: center !important;\n",
        "    font-size: 52px !important;\n",
        "    font-weight: 900 !important;\n",
        "    line-height: 1.15 !important;\n",
        "\n",
        "\n",
        "    text-shadow:\n",
        "        0 0 6px rgba(255, 210, 160, 0.75),\n",
        "        0 0 12px rgba(255, 180, 120, 0.55),\n",
        "        0 0 20px rgba(255, 150, 90, 0.35) !important;\n",
        "\n",
        "    animation: softGlow 3s ease-in-out infinite alternate !important;\n",
        "}\n",
        "\n",
        "@keyframes softGlow {\n",
        "    from {\n",
        "        text-shadow:\n",
        "            0 0 6px rgba(255, 210, 160, 0.75),\n",
        "            0 0 12px rgba(255, 180, 120, 0.55),\n",
        "            0 0 20px rgba(255, 150, 90, 0.35);\n",
        "    }\n",
        "    to {\n",
        "        text-shadow:\n",
        "            0 0 10px rgba(255, 220, 180, 0.9),\n",
        "            0 0 18px rgba(255, 190, 140, 0.7),\n",
        "            0 0 28px rgba(255, 160, 100, 0.5);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ ANIMATED DESCRIPTION ------------------\n",
        "animated_description = \"\"\"\n",
        "<div class=\"typing\" style=\"color:white; font-size:20px; margin-bottom:15px;\">\n",
        "This AI model predicts the category of any news article instantly...\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "footer_html = \"\"\"\n",
        "<div class=\"footer\">\n",
        "Project by <b>Ishleen Kaur</b> (Roll No: <b>80025340021</b>)\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ------------------ FULL GRADIO APP WITH TABS ------------------\n",
        "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.HTML(\"<h1>CateX</h1>\")\n",
        "    gr.HTML(animated_description)\n",
        "    gr.HTML(footer_html)\n",
        "\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # ------- TAB 1: MANUAL INPUT -------\n",
        "        with gr.TabItem(\"Manual Prediction\"):\n",
        "            title_input = gr.Textbox(label=\"News Title\")\n",
        "            summary_input = gr.Textbox(label=\"News Summary\", lines=5)\n",
        "            output_label = gr.Label(label=\"Predicted Category\")\n",
        "            button = gr.Button(\"Predict\")\n",
        "            button.click(classify_news, [title_input, summary_input], output_label)\n",
        "\n",
        "        # ------- TAB 2: CSV UPLOAD -------\n",
        "        with gr.TabItem(\"üìÅ Upload CSV\"):\n",
        "            csv_file = gr.File(label=\"Upload CSV (must contain Title & Summary)\")\n",
        "            csv_output_table = gr.DataFrame(label=\"Results Preview\")\n",
        "            csv_download = gr.File(label=\"Download Result CSV\")\n",
        "\n",
        "            csv_button = gr.Button(\"Process CSV\")\n",
        "            csv_button.click(\n",
        "                classify_csv,\n",
        "                inputs=csv_file,\n",
        "                outputs=[csv_output_table, csv_download]\n",
        "            )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "adI0SZFtgwBU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-P8WdL5j0n1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}